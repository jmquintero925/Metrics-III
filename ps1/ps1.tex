\documentclass{article}
\include{setup.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Fill in the appropriate information below
\lhead{Problem Set 1}
\rhead{Empirical Analysis} 
\title{Problem Set 1}
\author{Alex Weinberg \and Isaac Norwich \and Jose M. Quintero}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% QUESTION 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 1}
Consider an i.i.d. sample $\left\{Y_{i}, D_{i}\right\}_{i=1}^{n}$, where $Y_{i}$ is future earnings and $D_{i} \in\{0,1\}$ indicates whether an individual received a scholarship to attend college. Let $Y_{i 0}$ and $Y_{i 1}$ represent the potential outcomes corresponding to the events $D_{i}=0$ and $D_{i}=1$, respectively.

\begin{problem}{a}
 Compute $\left(\beta_{0}, \beta_{1, i}, U_{i}\right)$ so that $Y_{i}=\beta_{0}+\beta_{1, i} D_{i}+U_{i}$, where $E\left(U_{i}\right)=0$. How would you interpret the random coefficient $\beta_{1, i}$ ? Is it identified for any individual $i$ ? Explain. 
\end{problem}
\begin{solution}
Recall that we can manipulate the definition of the outcome variable.
\begin{align*}
Y_{i} & =Y_{1i}D_{i}+Y_{0i}\left(1-D_{i}\right)\\
 & =Y_{0i}+D_{i}\left(Y_{1i}-Y_{0i}\right)
\end{align*}

So then I can define
\begin{align*}
\beta_{1,i} & =Y_{1i}-Y_{0i}\\
\beta_{0} & =E\left[Y_{0,i}\right]\\
U_{i} & =Y_{0,i}-E\left[Y_{0,i}\right]
\end{align*}

note that $E\left[U_{i}\right]=0$ by definition. Finally stack.

\[
\ensuremath{Y_{i}=\beta_{0}+\beta_{1,i}D_{i}+U_{i}}
\]

The interpretation of $\beta_{i1}$ is the \emph{individual treatment
effect. }Note that we cannot recover $\beta_{i1}$ because we cannot
disentangle the individual elements $\beta_{1,i}$ and $U_{i}$. However
we can recover averages. 
\end{solution}
\begin{problem}{b}
Is $\kappa=E\left(Y_{i} \mid D_{i}=1\right)-E\left(Y_{i} \mid D_{i}=0\right)$ identified? When is $\kappa$ equal to ATT, ATUT, or ATE? \end{problem}
\begin{solution}
Yes $\kappa$ is identified. We observe
\begin{align*}
E\left[Y_{i}|D_{i}=1\right] & =E\left[Y_{1i}D_{i}+Y_{0i}\left(1-D_{i}\right)|D_{i}=1\right]\\
 & =E\left[Y_{1i}|D_{i}=1\right]
\end{align*}

We also observe
\begin{align*}
E\left[Y_{i}|D_{i}=0\right] & =E\left[Y_{1i}D_{i}+Y_{0i}\left(1-D_{i}\right)|D_{i}=0\right]\\
 & =E\left[Y_{0i}|D_{i}=0\right]
\end{align*}

So our parameter
\begin{align*}
\kappa & :=E\left[Y_{i}|D_{i}=1\right]-E\left[Y_{i}|D_{i}=0\right]\\
 & =E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]
\end{align*}

If treatment is uncorrelated with potential outcomes then then our
parameter is equal to the ATE
\begin{align*}
\\
\kappa & :=E\left[Y_{i}|D_{i}=1\right]-E\left[Y_{i}|D_{i}=0\right]\\
 & =E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]\\
 & =E\left[Y_{1i}\right]-E\left[Y_{0i}\right]\tag{\text{because \ensuremath{Y_{1i},Y_{0i}}\ensuremath{\perp D_{i}}}}\\
 & =E\left[Y_{1i}-Y_{0i}\right]\tag{\text{by linearity of expectations}}
\end{align*}

If treatment is always uncorrelated with potential outcomes then $\kappa=ATE=ATUT=ATT$.
But if treatment usually is selected into then the ATE will not necessarily
equal the ATUT, ATT. I suppose the way to figure out the ATT would
be to randomly assign the scholarship/control among students who apply.
ATUT could be assessed by randomly assigning scholarship/control among
students who do not apply to the scholarship. 
\end{solution}

\begin{problem}{c}Suppose the scholarship is only given to high-achieving students, who are already more likely to have higher earnings. Will $\kappa$ overstate or understate the ATT and the ATUT?
\end{problem}
\begin{solution}
Our observed parameter
\begin{align*}
\kappa & :=E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]\\
 & =\underbrace{E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=1\right]}_{\text{ATT}}+\underbrace{E\left[Y_{0i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]}_{\text{selection bias}}
\end{align*}

So if the scholarship is givent to high achieving students - who have
high $Y_{0i}$ then selection bias will be positive. This means that
$\kappa$ overstates the ATT. Similarly for ATUT,
\begin{align*}
\kappa & :=E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]\\
 & =\underbrace{E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{1i}|D_{i}=0\right]}_{\text{selection}}+\underbrace{E\left[Y_{1i}|D_{i}=0\right]-E\left[Y_{0i}|D_{i}=0\right]}_{\text{ATUT}}
\end{align*}

Assuming the selection is still positive, this means that $\kappa>ATUT$.
\end{solution}
\begin{problem}{d} Suppose the scholarship is given based on financial need, so that its recipients are more positively affected by $D_{i}$ than other students are. How do you expect the ATT, ATUT, and ATE will compare in magnitude? Provide intuition behind your comparisons.
\end{problem}
\begin{solution}
Suppose now that the scholarship is targeted towards students most
likely to benefit from the program. In other words,

\[
E\left[Y_{1i}-Y_{0i}|D_{i}=1\right]>E\left[Y_{1i}-Y_{0i}|D_{i}=0\right]
\]

Notice here that the ATT is larger that the ATUT. Since the ATE is
a weighted average of the ATT and ATUT we get
\[
\text{ATT}>\text{ATE}>\text{ATUT}
\]
The intuition is obvious, if the program targets those most likely
to benefit then the treatment effect of the program will be larger
than the treatment effect for a student randomly drawn from the population.
\end{solution}

\begin{problem}{e} For this part only, suppose $Y_{i 1}-Y_{i 0}$ equals a constant $c$. Is the slope estimator from an OLS regression of $Y_{i}$ on $\left(1, D_{i}\right)$ consistent for $c$ ? Make sure to derive the limit of this estimator, and then argue whether this limit equals $c$. Offer some intuition behind your results. 
\end{problem}
\begin{solution}
I suppress the $i$ subscript for readability. Recall that 
\begin{align*}
\beta^{OLS} & =\frac{Cov\left[D,Y\right]}{Var\left[D\right]}\\
 & =\frac{E\left[DY\right]-E\left[D\right]E\left[Y\right]}{E\left[D^{2}\right]-E\left[D\right]^{2}}
\end{align*}

I split into parts for algebra. Because $D\in\left\{ 0,1\right\} $,
$D^{2}=D$ and so the denominator becomes
\begin{align*}
E\left[D^{2}\right]-E\left[D\right]^{2} & =E\left[D\right]-E\left[D\right]^{2}\\
 & =E\left[D\right]\left(1-E\left[D\right]\right)
\end{align*}

Plug the definition of $Y$ into the numerator to get 
\begin{align*}
E\left[DY\right]-E\left[D\right]E\left[Y\right] & =E\left[D\left(Y_{1}D+Y_{0}\left(1-D\right)\right)\right]-E\left[D\right]E\left[Y_{1}D+Y_{0}\left(1-D\right)\right]\\
 & =E\left[\left(Y_{1}D^{2}+Y_{0}D\left(1-D\right)\right)\right]-E\left[D\right]\left(E\left[Y_{1}D\right]+E\left[Y_{0}\left(1-D\right)\right]\right)\\
 & =E\left[Y_{1}D\right]-E\left[D\right]\left(E\left[Y_{1}D\right]+E\left[Y_{0}\left(1-D\right)\right]\right)\\
 & =E\left[Y_{1}|D=1\right]E\left[D\right]-E\left[D\right]\left(E\left[Y_{1}|D=1\right]E\left[D\right]+E\left[Y_{0}|D=0\right]E\left[1-D\right]\right)\\
 & =E\left[D\right]\left(E\left[Y_{1}|D=1\right]-E\left[Y_{1}|D=1\right]E\left[D\right]-E\left[Y_{0}|D=0\right]\left(1-E\left[D\right]\right)\right)\\
 & =E\left[D\right]\left(\left(1-E\left[D\right]\right)E\left[Y_{1}|D=1\right]-E\left[Y_{0}|D=0\right]\left(1-E\left[D\right]\right)\right)\\
 & =E\left[D\right]\left(1-E\left[D\right]\right)\left(E\left[Y_{1}|D=1\right]-E\left[Y_{0}|D=0\right]\right)
\end{align*}

Combine numerator and denominator to get
\begin{align*}
\beta^{OLS} & =\frac{E\left[D\right]\left(1-E\left[D\right]\right)\left(E\left[Y_{1}|D=1\right]-E\left[Y_{0}|D=0\right]\right)}{E\left[D\right]\left(1-E\left[D\right]\right)}\\
 & =E\left[Y_{1}|D=1\right]-E\left[Y_{0}|D=0\right]\\
 & =:\kappa
\end{align*}

This is of course our parameter of interest. We know from all of the
usual OLS results that the sample estimator is consistent, unbiased,
and assymptotically normal. So the answer is yes - OLS is consistent
for $\kappa$. In general, we know that linear regression has the
interpretation as the conditional expectation function. In this case,
the regression is fully saturated - there is a regressor for every
possible outcome (trivially here because there is either treated or
not treated) so the linear regression is exactly a comparison of two
conditional expectation functions. 

\end{solution}

\begin{problem}{f}Suppose that treatment $D_{i}$ is randomized so that $P\left(D_{i}=1\right)=0.5$. Show that $\kappa=$ ATE.
\end{problem}
\begin{solution}
Our parameter is 
\begin{align*}
\kappa & :=E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]\\
 & =E\left[Y_{1i}\right]-E\left[Y_{0i}\right]\\
 & =E\left[Y_{1i}-Y_{0i}\right]
\end{align*}
where the second equality comes from random assignment. The third
equality comes from the linearity of expectations. 
\end{solution}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% QUESTION 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 2}

Consider the following Roy Model setup
\textbf{\begin{align*}
\begin{aligned}
    Y_{1} &=U_{1} \\
    Y_{0} &=U_{0} \\
    \left(\begin{array}{c}
    U_{1} \\
    U_{0}
    \end{array}\right) & \sim \mathcal{N}\left(\left(\begin{array}{l}
    0 \\
    0
    \end{array}\right),\left(\begin{array}{cc}
    \sigma^{2} & \rho \sigma \\
    \rho \sigma & 1
    \end{array}\right)\right) \\
    D &=\mathbbm{1}\left[U_{1}>U_{0}\right] \\
    Y &=D Y_{1}+(1-D) Y_{0}=\underbrace{\left(Y_{1}-Y_{0}\right)}_{\beta} D+Y_{0}
\end{aligned}
\end{align*}}

\begin{problem}{a}
Derive the expression for $\beta_{O L S}$. What treatment effect does it correspond to when $D \perp\left(Y_{1}, Y_{0}\right)$ ?
\end{problem}
\begin{solution}
The expression for $\beta_\text{OLS}$ can be derived as:
\begin{align}
\begin{split}
    \beta
\end{split}
\end{align}
\end{solution}


\begin{problem}{b}
Derive expressions for ATT and ATUT. Comment on their relative magnitudes and signs.
\end{problem}
\begin{solution}

\end{solution}
\begin{problem}{c}
What is ATE in this case?
\end{problem}
\begin{solution}

\end{solution}
\begin{problem}{d}
Derive expressions for $\frac{\partial A T T}{\partial \rho}, \frac{\partial A T U T}{\partial \rho}$, and $\frac{\partial \beta_{O L S}}{\partial \rho}$. Do the same for $\frac{\partial A T T}{\partial \sigma}, \frac{\partial A T U T}{\partial \sigma}$, and $\frac{\partial \beta_{O L S}}{\partial \sigma}$. Make sure to provide a simple intuitive explanation behind each of your results.
\end{problem}
\begin{solution}

\end{solution}
\begin{problem}{e}
Set $\sigma=2$ and $\rho=0.5$. Draw $N=10,000$ pairs of $\left(U_{0}, U_{1}\right)$. Compute ATE, ATT, ATUT and $\beta_{O L S}$. (Note: you have all the counterfactuals in this set up-take advantage of this fact). Compute $\mathbb{E}[Y \mid D=1]-\mathbb{E}[Y \mid D=0]$. What parameter does it correspond to? Repeat this for $(\sigma, \rho) \in\{(2,0),(2,-0.5)\}$. Fix $\rho=0.5$ and vary $\sigma$ to verify your findings in (d).
\end{problem}
\begin{solution}
    See Topic 2 PDF slide 5: the expectation diff thing is basically ATT plus selection bias or ATU plus selection bias.
\end{solution}
\begin{problem}{f}
Claim: in this setup, $D \perp\left(Y_{1}, Y_{0}\right)$ if and only if $\rho=0$. Argue whether this claim is true or not. Justify using the results from (e).
\end{problem}
\begin{solution}

\end{solution}


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% QUESTION 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 3}
The purpose of this exercise is to introduce you to the bootstrapping procedure. In previous classes, you learned that, given i.i.d. data, the OLS estimator $\hat{\beta}$ will satisfy:
\begin{align*}
\hat{\beta} \stackrel{p}{\rightarrow} \beta=\mathbb{E}\left[X_{i} X_{i}^{\prime}\right]^{-1} \mathbb{E}\left[X_{i} Y_{i}\right] \quad \text { and } \quad \sqrt{N}(\hat{\beta}-\beta) \stackrel{d}{\rightarrow} \mathcal{N}(0, V),
\end{align*}
for some matrix $V$. Define $\hat{V}$ and $\operatorname{se}\left(\hat{\beta}_{k}\right)$ so that $\hat{V} \stackrel{p}{\rightarrow} V$ and $\operatorname{se}\left(\hat{\beta}_{k}\right)=\sqrt{\frac{1}{N} \operatorname{diag}(\hat{V})_{k}}$.
Monte Carlo Simulations
Consider the model $Y_{i}=X_{i}^{\prime} \beta+U_{i}$, where $U_{i} \mid X_{i} \stackrel{i . i . d}{\sim} \mathcal{N}\left(0, \sigma^{2}\right)$.
\begin{problem}{a}
Set $\beta=(2,3)^{\prime}$ and $\sigma^{2}=4$. Generate $N=10,000$ values for $X \in \mathbb{R}^{2}$ (one constant and one covariate). Given your value of $\sigma^{2}$, draw $U$ 's (they may be independent of $X$ ). Finally, compute $Y$ 's. Estimate $\hat{\beta}$ and its standard errors from your data using OLS.
\end{problem}
\begin{solution}
To see the full procedure see the attached code. The results presented in Table \ref{table:ea3:ps1:q3a:tab1}. These results are calculated using the OLS function from Matlab. However, the code fully calculates the point estimates and standard errors using the formulas and compares it to the estimates done by the build-in function. 
\input{Tables/q3a_tab1}
Note that the point estimates are almost identical to the true value. 
\end{solution}
\FloatBarrier
\begin{problem}{b}
Using $X, \beta$ and $\sigma^{2}$ from part (a), draw $S=10,000$ of $U^{(s)}$ (an $N \times 1$ vector for each $s$ ) and the corresponding $Y^{(s)}$. For each $Y^{(s)}$, compute the OLS estimator $\hat{\beta}^{(s)}$. Then, compute:
    \begin{align*}
        \sqrt{\widehat{\operatorname{Var}}\left[\hat{\beta}_{k}^{(s)} \mid X_{1}, \ldots, X_{N}\right]}=\sqrt{\frac{1}{S} \sum_{s=1}^{S}\left(\hat{\beta}_{k}^{(s)}\right)^{2}-\left(\frac{1}{S} \sum_{s=1}^{S} \hat{\beta}_{k}^{(s)}\right)^{2}} \stackrel{p}{\longrightarrow} \operatorname{se}\left(\hat{\beta}_{k} \mid X_{1}, \ldots, X_{N}\right)
    \end{align*}
    Justify the $" \stackrel{p}{\longrightarrow}$ " in the line above. Plot a histogram for the first component of $\beta^{(s)}$.
\end{problem}
\begin{solution}
For this problem it is important to highlight that $X$ is simulated from a standard normal distribution. The Montecarlo estimation results are presented in Table \ref{table:ea3:ps1:q3a:tab2}. 
\input{Tables/q3a_tab2} \\ 
The Montecarlo simulation cleans up some of the error induced by $U_i$ as our estimates are even closer to the true underlying value and the standard error is still the same. The convergence in probability of the estimated standard error to the theoretical is an application of both the Law of Large Numbers and the continuous mapping theorem. Each Montecarlo estimated parameter $\hat{\beta}_k^{(s)}$ can be interpreted as a draw from the distribution of $\hat{\beta}_k$. Since $\hat{\beta}_k$ has a normal distribution then it has a second moment and 
\begin{align*}
    \frac{1}{S} \sum_{s=1}^{S}\left(\hat{\beta}_{k}^{(s)}\right)^{2}&\stackrel{p}{\longrightarrow}  \E[\hat{\beta}_k^2] & 
    \left(\frac{1}{S} \sum_{s=1}^{S} \hat{\beta}_{k}^{(s)}\right)^{2}&\stackrel{p}{\longrightarrow} \E[\hat{\beta}_k]^2
\end{align*}
by both the LLN and the continuous mapping theorem. To justify the claim that each $\hat{\beta}_k^{(s)}$ consider Figure \ref{ea3:ps1:q3a:fig1}. Note that over the 10,000 observations the implied histogram matches the pdf of the theoretical distribution. The theoretical distribution can be easily calculated as we selected $X_1\sim\mathcal{N}(0,1)$. Thus, the theoretical matrix $V$ is 
\begin{equation*}
    V = \E[X'X]\sigma^2 = \E\begin{bmatrix}
    1 & X_1 \\ X_1 & X_1^2
    \end{bmatrix}\sigma^2 = 
    \begin{bmatrix}
    4 & 0 \\ 0 & 4
    \end{bmatrix} 
\end{equation*}
Then for $N$ sufficiently large, the distribution of $\hat{\beta}$ is 
\begin{equation*}
    \hat{\beta}\sim\mathcal{N}\left(\begin{bmatrix}
    2 \\ 3
    \end{bmatrix},\begin{bmatrix}
    \sfrac{4}{N} & 0 \\ 0 & \sfrac{4}{N}
    \end{bmatrix}\right)
\end{equation*}
\begin{figure}[htb]
    \centering
    \caption{OLS Estimate Distribution from Montecarlo}
    \label{ea3:ps1:q3a:fig1}
    \includegraphics[width=0.45\textwidth]{Figures/p3qa.pdf}
\end{figure}

\end{solution}
\FloatBarrier
\underline{\hspace{\textwidth}}
\textbf{Nonparametric Bootstrap}
The rough idea behind bootstrapping is that we expect a large sample of observed data to behave like the wider population. Rather than drawing independent samples, which may be too costly or time-consuming, we can make inferences about our estimators by re-sampling from the original data. The rationale behind this idea is that the empirical distribution is close to the population distribution as the sample size becomes large. Through the bootstrap procedure, we can conduct inference about our estimates without specifying the data generating process. Consider the RCT setup: $Y=D Y_{1}+(1-D) Y_{0}$, where $D \perp\left(Y_{1}, Y_{0}\right)$.
\begin{problem}{a}
Define constant values for $Y_{1}=5+U_{1}$ and $Y_{0}=2+U_{2}$, where $U_{1}, U_{2} \stackrel{i . i . d}{\sim} N(0,1)$. Assign treatment $D \in\{0,1\}$ randomly (setting $P(D=1)=0.5$ is a good choice) to $N=10,000$ individuals. Estimate $E\left(Y_{1}-Y_{0}\right)$ using OLS and compute the standard errors. Then, argue that OLS gives consistent coefficient estimates. Why is $D \perp\left(Y_{1}, Y_{0}\right)$ important here?
\end{problem}
\begin{solution}
Begin by defining $Y$ to estimate the OLS regression as 
\begin{align*}
    Y   &= Y_1D + (1-D)Y_0 \\ 
        &= (Y_1-Y_0)D + Y_0
\end{align*}
By taking expectations it yields that 
\begin{equation*}
    \E[Y\vert D] = D\E[Y_1-Y_0\vert D] + \E[Y_0\vert D]
\end{equation*}
which implies that $\hat{\beta}_0=\E[Y_0\vert D] = \E[Y_0]$ using the independence assumption. Similarly, 
\begin{align*}
\hat{\beta}_1 & =\E[Y_1-Y_0\vert D] \\
&= \E[Y_1\vert D=1] - \E[Y_0\vert D=0] \\ 
&= \E[Y_1]-\E[Y_0] = \E[Y_1-Y_0] 
\end{align*}
But note that the independence assumption is key for the identification of the unconditional effect of the RCT. If $D\not\perp (Y_1,Y_0)$ then $\E[Y_1\vert D=1] - \E[Y_0\vert D=0]\neq \E[Y_1-Y_0]$. The results of the OLS regression are presented in Table \ref{table:ea3:ps1:q3b:tab1}.
\input{Tables/q3b_tab1} 
Once again, the OLS results are close to the population value. 
\end{solution}
\begin{problem}{b}
From the initial data that you generated, draw $N=10,000$ pairs of $\left(Y_{i}, D_{i}\right)$, choosing each of the original data pairs with probability $\frac{1}{N}$ (with replacement). Repeat this procedure a total of $S=10,000$ times. You should have $S$ samples of $N=$ observations, each generated from the original sample. Repeat the computations from the Monte Carlo Simulations part, making sure to compute $\left(\widehat{\operatorname{Var}}\left[\hat{\beta}^{(s)}\right]\right)^{1 / 2}$ and to plot a histogram for $\hat{\beta}^{(s)}$. What would happen if you drew $Y$ and $D$ independently from the original sample, instead of as a pair?
\end{problem}
\begin{solution}
The results of the Bootstrap are presented in Table \ref{table:ea3:ps1:q3b:tab1}. One key feature of the Bootstrap exercise is that the distribution generated by Bootstrapping is not the theoretical one but in reality the distribution around the OLS estimates. 
\input{Tables/q3b_tab2}
To see this clearly, refer to Figure \ref{ea3:ps1:q3b:fig1}. The Empirical distribution is constructed using a normal distribution center around the OLS estimate and the variance is the square of the estimated standard error calculated by OLS. 
\begin{figure}[htb]
    \centering
    \caption{OLS Estimate Distribution from Bootstrapping}
    \label{ea3:ps1:q3b:fig1}
    \includegraphics[width=0.45\textwidth]{Figures/p3qb.pdf}
\end{figure}
\end{solution}
\end{document}
