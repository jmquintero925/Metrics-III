\documentclass{article}
\include{setup.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Fill in the appropriate information below
\lhead{Alex Weinberg, Isaac Norwich, JosÃ© Quintero}
\rhead{Empirical Analysis III} 
\chead{Problem Set 1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% QUESTION 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 1}
\textbf{Consider an i.i.d. sample $\left\{Y_{i}, D_{i}\right\}_{i=1}^{n}$, where $Y_{i}$ is future earnings and $D_{i} \in\{0,1\}$ indicates whether an individual received a scholarship to attend college. Let $Y_{i 0}$ and $Y_{i 1}$ represent the potential outcomes corresponding to the events $D_{i}=0$ and $D_{i}=1$, respectively.}

IID sample. $Y_{i}$ is future earnings. $D_{i}\in\left\{ 0,1\right\} $
is a binary treatment - recieving scholarship. 

\begin{problem}{1}
(a) Compute $\left(\beta_{0}, \beta_{1, i}, U_{i}\right)$ so that $Y_{i}=\beta_{0}+\beta_{1, i} D_{i}+U_{i}$, where $E\left(U_{i}\right)=0$. How would you interpret the random coefficient $\beta_{1, i}$ ? Is it identified for any individual $i$ ? Explain. 
\end{problem}

Recall that we can manipulate the definition of the outcome variable.
\begin{align*}
Y_{i} & =Y_{1i}D_{i}+Y_{0i}\left(1-D_{i}\right)\\
 & =Y_{0i}+D_{i}\left(Y_{1i}-Y_{0i}\right)
\end{align*}

So then I can define
\begin{align*}
\beta_{1,i} & =Y_{1i}-Y_{0i}\\
\beta_{0} & =E\left[Y_{0,i}\right]\\
U_{i} & =Y_{0,i}-E\left[Y_{0,i}\right]
\end{align*}

note that $E\left[U_{i}\right]=0$ by defintion. Finally stack.

\[
\ensuremath{Y_{i}=\beta_{0}+\beta_{1,i}D_{i}+U_{i}}
\]

The interpretation of $\beta_{i1}$ is the \emph{individual treatment
effect. }Note that we cannot recover $\beta_{i1}$ because we cannot
disentangle the individual elements $\beta_{1,i}$ and $U_{i}$. However
we can recover averages. 

\begin{problem}{b}
Is $\kappa=E\left(Y_{i} \mid D_{i}=1\right)-E\left(Y_{i} \mid D_{i}=0\right)$ identified? When is $\kappa$ equal to ATT, ATUT, or ATE? \end{problem}

Yes $\kappa$ is identified. We observe
\begin{align*}
E\left[Y_{i}|D_{i}=1\right] & =E\left[Y_{1i}D_{i}+Y_{0i}\left(1-D_{i}\right)|D_{i}=1\right]\\
 & =E\left[Y_{1i}|D_{i}=1\right]
\end{align*}

We also observe
\begin{align*}
E\left[Y_{i}|D_{i}=0\right] & =E\left[Y_{1i}D_{i}+Y_{0i}\left(1-D_{i}\right)|D_{i}=0\right]\\
 & =E\left[Y_{0i}|D_{i}=0\right]
\end{align*}

So our parameter
\begin{align*}
\kappa & :=E\left[Y_{i}|D_{i}=1\right]-E\left[Y_{i}|D_{i}=0\right]\\
 & =E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]
\end{align*}

If treatment is uncorrelated with potential outcomes then then our
parameter is equal to the ATE
\begin{align*}
\\
\kappa & :=E\left[Y_{i}|D_{i}=1\right]-E\left[Y_{i}|D_{i}=0\right]\\
 & =E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]\\
 & =E\left[Y_{1i}\right]-E\left[Y_{0i}\right]\tag{\text{because \ensuremath{Y_{1i},Y_{0i}}\ensuremath{\perp D_{i}}}}\\
 & =E\left[Y_{1i}-Y_{0i}\right]\tag{\text{by linearity of expectations}}
\end{align*}

If treatment is always uncorrelated with potential outcomes then $\kappa=ATE=ATUT=ATT$.
But if treatment usually is selected into then the ATE will not necessarily
equal the ATUT, ATT. I suppose the way to figure out the ATT would
be to randomly assign the scholarship/control among students who apply.
ATUT could be assessed by randomly assigning scholarship/control among
students who do not apply to the scholarship. \begin{problem}{c}Suppose the scholarship is only given to high-achieving students, who are already more likely to have higher earnings. Will $\kappa$ overstate or understate the ATT and the ATUT?
\end{problem}

Our observed parameter
\begin{align*}
\kappa & :=E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]\\
 & =\underbrace{E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=1\right]}_{\text{ATT}}+\underbrace{E\left[Y_{0i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]}_{\text{selection bias}}
\end{align*}

So if the scholarship is givent to high achieving students - who have
high $Y_{0i}$ then selection bias will be positive. This means that
$\kappa$ overstates the ATT. Similarly for ATUT,
\begin{align*}
\kappa & :=E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]\\
 & =\underbrace{E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{1i}|D_{i}=0\right]}_{\text{selection}}+\underbrace{E\left[Y_{1i}|D_{i}=0\right]-E\left[Y_{0i}|D_{i}=0\right]}_{\text{ATUT}}
\end{align*}

Assuming the selection is still positive, this means that $\kappa>ATUT$.\begin{problem}{d} Suppose the scholarship is given based on financial need, so that its recipients are more positively affected by $D_{i}$ than other students are. How do you expect the ATT, ATUT, and ATE will compare in magnitude? Provide intuition behind your comparisons.
\end{problem}

Suppose now that the scholarship is targeted towards students most
likely to benefit from the program. In other words,

\[
E\left[Y_{1i}-Y_{0i}|D_{i}=1\right]>E\left[Y_{1i}-Y_{0i}|D_{i}=0\right]
\]

Notice here that the ATT is larger that the ATUT. Since the ATE is
a weighted average of the ATT and ATUT we get
\[
ATT>ATE>ATUT
\]

The intuition is obvious, if the program targets those most likely
to benefit then the treatment effect of the program will be larger
than the treatment effect for a student randomly drawn from the population.
\begin{problem}{e} For this part only, suppose $Y_{i 1}-Y_{i 0}$ equals a constant $c$. Is the slope estimator from an OLS regression of $Y_{i}$ on $\left(1, D_{i}\right)$ consistent for $c$ ? Make sure to derive the limit of this estimator, and then argue whether this limit equals $c$. Offer some intuition behind your results. 
\end{problem}

This is simple. We did this with Azeem. I'll type up tomorrow. \begin{problem}{f}Suppose that treatment $D_{i}$ is randomized so that $P\left(D_{i}=1\right)=0.5$. Show that $\kappa=$ ATE.
\end{problem}

Our parameter is 

\begin{align*}
\kappa & :=E\left[Y_{1i}|D_{i}=1\right]-E\left[Y_{0i}|D_{i}=0\right]\\
 & =E\left[Y_{1i}\right]-E\left[Y_{0i}\right]\\
 & =E\left[Y_{1i}-Y_{0i}\right]
\end{align*}

where the second equality comes from random assignment. The third
equality comes from the linearity of expectations. 




\begin{enumerate}[(a), wide, labelwidth=!, labelindent=0pt]
    \item \textbf{Compute $\left(\beta_{0}, \beta_{1, i}, U_{i}\right)$ so that $Y_{i}=\beta_{0}+\beta_{1, i} D_{i}+U_{i}$, where $E\left(U_{i}\right)=0$. How would you interpret the random coefficient $\beta_{1, i}$ ? Is it identified for any individual $i$ ? Explain.}
    
    \item \textbf{Is $\kappa=E\left(Y_{i} \mid D_{i}=1\right)-E\left(Y_{i} \mid D_{i}=0\right)$ identified? When is $\kappa$ equal to ATT, ATUT, or ATE?}
    
    \item \textbf{Suppose the scholarship is only given to high-achieving students, who are already more likely to have higher earnings. Will $\kappa$ overstate or understate the ATT and the ATUT?}
    
    \item \textbf{Suppose the scholarship is given based on financial need, so that its recipients are more positively affected by $D_{i}$ than other students are. How do you expect the ATT, ATUT, and ATE will compare in magnitude? Provide intuition behind your comparisons.}
    
    \item \textbf{For this part only, suppose $Y_{i 1}-Y_{i 0}$ equals a constant $c$. Is the slope estimator from an OLS regression of $Y_{i}$ on $\left(1, D_{i}\right)$ consistent for $c$ ? Make sure to derive the limit of this estimator, and then argue whether this limit equals $c$. Offer some intuition behind your results.}
    
    \item \textbf{Suppose that treatment $D_{i}$ is randomized so that $P\left(D_{i}=1\right)=0.5$. Show that $\kappa=$ ATE.}    
    
\end{enumerate}






\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% QUESTION 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 2}

Consider the following Roy Model setup
\textbf{\begin{align*}
\begin{aligned}
    Y_{1} &=U_{1} \\
    Y_{0} &=U_{0} \\
    \left(\begin{array}{c}
    U_{1} \\
    U_{0}
    \end{array}\right) & \sim \mathcal{N}\left(\left(\begin{array}{l}
    0 \\
    0
    \end{array}\right),\left(\begin{array}{cc}
    \sigma^{2} & \rho \sigma \\
    \rho \sigma & 1
    \end{array}\right)\right) \\
    D &=\mathbbm{1}\left[U_{1}>U_{0}\right] \\
    Y &=D Y_{1}+(1-D) Y_{0}=\underbrace{\left(Y_{1}-Y_{0}\right)}_{\beta} D+Y_{0}
\end{aligned}
\end{align*}}

\begin{enumerate}[(a), wide, labelwidth=!, labelindent=0pt]
    \item \textbf{Derive the expression for $\beta_{O L S}$. What treatment effect does it correspond to when $D \perp\left(Y_{1}, Y_{0}\right)$ ?}
    
    \item \textbf{Derive expressions for ATT and ATUT. Comment on their relative magnitudes and signs.}
    
    \item \textbf{What is ATE in this case?}
    
    \item \textbf{Derive expressions for $\frac{\partial A T T}{\partial \rho}, \frac{\partial A T U T}{\partial \rho}$, and $\frac{\partial \beta_{O L S}}{\partial \rho}$. Do the same for $\frac{\partial A T T}{\partial \sigma}, \frac{\partial A T U T}{\partial \sigma}$, and $\frac{\partial \beta_{O L S}}{\partial \sigma}$. Make sure to provide a simple intuitive explanation behind each of your results.}
    
    \item \textbf{Set $\sigma=2$ and $\rho=0.5$. Draw $N=10,000$ pairs of $\left(U_{0}, U_{1}\right)$. Compute ATE, ATT, ATUT and $\beta_{O L S}$. (Note: you have all the counterfactuals in this set up-take advantage of this fact). Compute $\mathbb{E}[Y \mid D=1]-\mathbb{E}[Y \mid D=0]$. What parameter does it correspond to? Repeat this for $(\sigma, \rho) \in\{(2,0),(2,-0.5)\}$. Fix $\rho=0.5$ and vary $\sigma$ to verify your findings in (d).}
    \item \textbf{Claim: in this setup, $D \perp\left(Y_{1}, Y_{0}\right)$ if and only if $\rho=0$. Argue whether this claim is true or not. Justify using the results from (e).}

\end{enumerate}






\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% QUESTION 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 3}
The purpose of this exercise is to introduce you to the bootstrapping procedure. In previous classes, you learned that, given i.i.d. data, the OLS estimator $\hat{\beta}$ will satisfy:
\begin{align*}
\hat{\beta} \stackrel{p}{\rightarrow} \beta=\mathbb{E}\left[X_{i} X_{i}^{\prime}\right]^{-1} \mathbb{E}\left[X_{i} Y_{i}\right] \quad \text { and } \quad \sqrt{N}(\hat{\beta}-\beta) \stackrel{d}{\rightarrow} \mathcal{N}(0, V),
\end{align*}
for some matrix $V$. Define $\hat{V}$ and $\operatorname{se}\left(\hat{\beta}_{k}\right)$ so that $\hat{V} \stackrel{p}{\rightarrow} V$ and $\operatorname{se}\left(\hat{\beta}_{k}\right)=\sqrt{\frac{1}{N} \operatorname{diag}(\hat{V})_{k}}$.
Monte Carlo Simulations
Consider the model $Y_{i}=X_{i}^{\prime} \beta+U_{i}$, where $U_{i} \mid X_{i} \stackrel{i . i . d}{\sim} \mathcal{N}\left(0, \sigma^{2}\right)$.
\begin{enumerate}[(a), wide, labelwidth=!, labelindent=0pt]
    \item \textbf{Set $\beta=(2,3)^{\prime}$ and $\sigma^{2}=4$. Generate $N=10,000$ values for $X \in \mathbb{R}^{2}$ (one constant and one covariate). Given your value of $\sigma^{2}$, draw $U$ 's (they may be independent of $X$ ). Finally, compute $Y$ 's. Estimate $\hat{\beta}$ and its standard errors from your data using OLS.}
    \item \textbf{Using $X, \beta$ and $\sigma^{2}$ from part (a), draw $S=10,000$ of $U^{(s)}$ (an $N \times 1$ vector for each $s$ ) and the corresponding $Y^{(s)}$. For each $Y^{(s)}$, compute the OLS estimator $\hat{\beta}^{(s)}$. Then, compute:
    \begin{align*}
        \sqrt{\widehat{\operatorname{Var}}\left[\hat{\beta}_{k}^{(s)} \mid X_{1}, \ldots, X_{N}\right]}=\sqrt{\frac{1}{S} \sum_{s=1}^{S}\left(\hat{\beta}_{k}^{(s)}\right)^{2}-\left(\frac{1}{S} \sum_{s=1}^{S} \hat{\beta}_{k}^{(s)}\right)^{2}} \stackrel{p}{\rightarrow} \operatorname{se}\left(\hat{\beta}_{k} \mid X_{1}, \ldots, X_{N}\right)
    \end{align*}
    Justify the $" \stackrel{p}{\rightarrow}$ " in the line above. Plot a histogram for the first component of $\beta^{(s)}$.}
\end{enumerate}
\textbf{Nonparametric Bootstrap}
The rough idea behind bootstrapping is that we expect a large sample of observed data to behave like the wider population. Rather than drawing independent samples, which may be too costly or time-consuming, we can make inferences about our estimators by re-sampling from the original data. The rationale behind this idea is that the empirical distribution is close to the population distribution as the sample size becomes large. Through the bootstrap procedure, we can conduct inference about our estimates without specifying the data generating process. Consider the RCT setup: $Y=D Y_{1}+(1-D) Y_{0}$, where $D \perp\left(Y_{1}, Y_{0}\right)$.
\begin{enumerate}[(a), wide, labelwidth=!, labelindent=0pt]
\item \textbf{Define constant values for $Y_{1}=5+U_{1}$ and $Y_{0}=2+U_{2}$, where $U_{1}, U_{2} \stackrel{i . i . d}{\sim} N(0,1)$. Assign treatment $D \in\{0,1\}$ randomly (setting $P(D=1)=0.5$ is a good choice) to $N=10,000$ individuals. Estimate $E\left(Y_{1}-Y_{0}\right)$ using OLS and compute the standard errors. Then, argue that OLS gives consistent coefficient estimates. Why is $D \perp\left(Y_{1}, Y_{0}\right)$ important here?}
\item \textbf{From the initial data that you generated, draw $N=10,000$ pairs of $\left(Y_{i}, D_{i}\right)$, choosing each of the original data pairs with probability $\frac{1}{N}$ (with replacement). Repeat this procedure a total of $S=10,000$ times. You should have $S$ samples of $N=$ observations, each generated from the original sample. Repeat the computations from the Monte Carlo Simulations part, making sure to compute $\left(\widehat{\operatorname{Var}}\left[\hat{\beta}^{(s)}\right]\right)^{1 / 2}$ and to plot a histogram for $\hat{\beta}^{(s)}$. What would happen if you drew $Y$ and $D$ independently from the original sample, instead of as a pair?}
\end{enumerate}
\end{document}
